{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inference.models.faster_rcnn_module import FasterRCNNModule\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import progressbar as pb\n",
    "\n",
    "model_path = Path('../lightning_logs/art_detection/frcnn_resnet50_img_576.pt')\n",
    "dataset_path = Path('/home/szymswiat/datasets/nih_dataset')\n",
    "\n",
    "state = torch.load(model_path.as_posix())\n",
    "hparams = OmegaConf.create(state['hparams'])\n",
    "classes = OmegaConf.to_object(hparams.dynamic.classes)\n",
    "\n",
    "model = FasterRCNNModule.create_from_state(state)\n",
    "model.model.roi_heads.score_thresh = 0.5\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data import nih_const\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from data.nih_dataset import NIHDataset\n",
    "import albumentations as A\n",
    "\n",
    "metadata = NIHDataset.parse_dataset_meta(\n",
    "    dataset_path=dataset_path,\n",
    "    split_type=NIHDataset.SPLIT_OFFICIAL_VAL_FROM_TEST,\n",
    "    classes=classes\n",
    ")\n",
    "img_size = hparams.phases[0].image_size\n",
    "transforms = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(mean=[0] * 3, std=[1] * 3, max_pixel_value=nih_const.MIN_MAX_VALUE[1], always_apply=True),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.2))\n",
    "\n",
    "test_set = NIHDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    input_df=metadata['test_df'],\n",
    "    mode=NIHDataset.BBOX_ART_DETECTION_MODE,\n",
    "    transforms=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from imageio.plugins.pillow import ndarray_to_pil\n",
    "\n",
    "for i, (img, y_true) in pb.progressbar(enumerate(test_set)):\n",
    "    if len(y_true) > 0:\n",
    "        true_boxes = torch.tensor(y_true)[..., :4]\n",
    "        true_labels = torch.tensor(y_true)[..., 4]\n",
    "    else:\n",
    "        true_boxes = torch.tensor([])\n",
    "        true_labels = torch.tensor([])\n",
    "    img_raw = (img * 255).to(torch.uint8)\n",
    "\n",
    "    y_pred = model(torch.unsqueeze(img, dim=0))\n",
    "    pred_boxes = y_pred[0]['boxes']\n",
    "    pred_labels = y_pred[0]['labels']\n",
    "    pred_scores = y_pred[0]['scores']\n",
    "\n",
    "    img_raw = draw_bounding_boxes(img_raw, true_boxes,\n",
    "                              labels=[classes[int(x)] for x in true_labels],\n",
    "                              colors=[(255, 0, 0)] * len(true_boxes))\n",
    "    img_raw = draw_bounding_boxes(img_raw, pred_boxes,\n",
    "                              labels=[f'{classes[int(l)]}: {float(p)}' for l, p in zip(pred_labels, pred_scores)],\n",
    "                              colors=[(0, 180, 0)] * len(pred_boxes))\n",
    "    ndarray_to_pil(img_raw.permute(1, 2, 0).detach().numpy()).save(f'images/{test_set.get_img_path(i).stem}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img, target = test_set[1]\n",
    "img = cv2.resize(img, (384, 384))\n",
    "img = np.float32(img / 255)\n",
    "img = np.moveaxis(img, [2], [0])\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "model_dir = Path('models')\n",
    "\n",
    "model.to_onnx((model_dir / 'frcnn.onnx').as_posix(),\n",
    "              input_sample=torch.zeros(1, 3, 384, 384),\n",
    "              opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "session = onnxruntime.InferenceSession('models/frcnn.onnx')\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_names = list(map(lambda x: x.name, session.get_outputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img, target = test_set[4]\n",
    "img = cv2.resize(img, (384, 384))\n",
    "img = np.float32(img / 255)\n",
    "img = np.moveaxis(img, [2], [0])\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "result = session.run(output_names, {input_name: img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
