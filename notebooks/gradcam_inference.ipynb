{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from modules.nih_efficient_net_v2_module import NIHEfficientNetV2Module\n",
    "from modules.nih_resnet_module import NIHResNetModule\n",
    "\n",
    "model_path = Path('../lightning_logs/resnet34_epoch=17_val_auroc=0.843_top_updated.pt')\n",
    "dataset_path = Path('/home/szymswiat/datasets/nih_dataset')\n",
    "img_size = 384\n",
    "\n",
    "state = torch.load(model_path.as_posix())\n",
    "hparams = OmegaConf.create(state['hparams'])\n",
    "thresholds = torch.tensor(state['thresholds'])\n",
    "classes = OmegaConf.to_object(hparams.dynamic.classes)\n",
    "\n",
    "if hparams.architecture == 'eff_net_v2':\n",
    "    model_class = NIHEfficientNetV2Module\n",
    "elif hparams.architecture == 'resnet':\n",
    "    model_class = NIHResNetModule\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "model = model_class.load_from_file(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "from data.nih_dataset import NIHDataset\n",
    "import albumentations as A\n",
    "import transforms as tfm\n",
    "\n",
    "metadata = NIHDataset.parse_dataset_meta(\n",
    "    dataset_path=dataset_path,\n",
    "    split_type=NIHDataset.SPLIT_OFFICIAL_VAL_FROM_TEST,\n",
    "    classes=classes\n",
    ")\n",
    "transforms = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    tfm.NormalizeAlb(NIHDataset.MIN_MAX_VALUE,\n",
    "                     mean=[NIHDataset.MEAN] * 3,\n",
    "                     std=[NIHDataset.STD] * 3),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_set = NIHDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    input_df=metadata['test_df'],\n",
    "    filter_by_positive_class=['Cardiomegaly']\n",
    "    # mode=NIHDataset.BBOX_ONLY_MODE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if hparams.architecture == 'eff_net_v2':\n",
    "    raise NotImplementedError()\n",
    "elif hparams.architecture == 'resnet':\n",
    "    # gradcam_target_layer = model.model.layer4[-3]\n",
    "    # gradcam_target_layer = model.model.layer3[-1]\n",
    "    gradcam_target_layer = model.model.layer4[0]\n",
    "else:\n",
    "    raise ValueError()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "\n",
    "cam = GradCAMPlusPlus(model=model, target_layer=gradcam_target_layer, use_cuda=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imageio.plugins.pillow import ndarray_to_pil\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import progressbar as pb\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from math import floor\n",
    "\n",
    "img_path = Path('images')\n",
    "img_path.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for i, (img_raw, y_true) in pb.progressbar(enumerate(test_set)):\n",
    "    inference_result = {}\n",
    "    classification_report = {cls: {'detected': False} for cls in classes}\n",
    "    inference_result['classification_report'] = classification_report\n",
    "\n",
    "    img = transforms(image=img_raw)['image'].float()\n",
    "    y_pred = model(torch.unsqueeze(img, dim=0)).squeeze(dim=0)\n",
    "\n",
    "    y_pred_positive = (y_pred >= thresholds).nonzero().squeeze(dim=1).detach().numpy()\n",
    "    pred_positive_count = len(y_pred_positive)\n",
    "\n",
    "    img_raw_path = test_set.get_img_path(i)\n",
    "    img_root = img_path / img_raw_path.stem\n",
    "    img_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    if pred_positive_count > 0:\n",
    "        images = torch.unsqueeze(img, dim=0).repeat((pred_positive_count, 1, 1, 1))\n",
    "\n",
    "        grayscale_cams = cam(images, target_category=y_pred_positive)\n",
    "        for j, class_idx in enumerate(y_pred_positive):\n",
    "            class_to_process = classes[class_idx]\n",
    "            img_debug_path = img_root / f'{class_to_process}.png'\n",
    "            img_heatmap_path  = img_root / f'{class_to_process}_heatmap.png'\n",
    "            img_heatmap_rgb_path  = img_root / (img_heatmap_path.stem + '_rgb.png')\n",
    "\n",
    "            report = classification_report[class_to_process]\n",
    "            report['detected'] = True\n",
    "            # report['confidence'] = floor(float(y_pred[class_idx]) * 1000) / 1000\n",
    "            report['heatmap_url'] = img_heatmap_rgb_path.name\n",
    "\n",
    "            single_cam = cv2.resize(grayscale_cams[j, :],\n",
    "                                    img_raw.shape[:2],\n",
    "                                    interpolation=cv2.INTER_NEAREST)\n",
    "            img_norm = torch.tensor(img_raw).float().numpy() / 255\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * single_cam), cv2.COLORMAP_JET)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # visualization = np.float32(heatmap) + img_raw\n",
    "            # visualization = np.uint8(visualization / np.max(visualization) * 255)\n",
    "            #\n",
    "            # ndarray_to_pil(np.concatenate((visualization, np.uint8(img_raw)), axis=1)).save(img_debug_path)\n",
    "            # ndarray_to_pil(np.uint8(single_cam * 255)).save(img_heatmap_path)\n",
    "            ndarray_to_pil(heatmap).save(img_heatmap_rgb_path)\n",
    "            ndarray_to_pil(img_raw).save(img_root / img_raw_path.name)\n",
    "\n",
    "    with open(img_root / 'inference_result.json', 'w') as f:\n",
    "        json.dump(inference_result, f, indent=4, sort_keys=True)\n",
    "\n",
    "    # if i == 10:\n",
    "    #     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}